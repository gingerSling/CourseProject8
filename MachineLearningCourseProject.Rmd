---
title: "MachineLearningCourseProject"
author: "Pablo Portillo Garrigues"
date: "31 de mayo de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r results="hide",message=FALSE,include=FALSE,echo=FALSE}
library(ggplot2)
library(car)
library(caret)
library(e1071)
library(lubridate)
library(ElemStatLearn)
library(gbm)
library(randomForest)
library(Hmisc)
library(dplyr)
library(elasticnet)
library(AppliedPredictiveModeling)
library(kernlab)
library(skimr)
```

## Summary

The porpouse of this paper is to decide, using machine learning techniques, in what way was carried out the excercise. The outcome variable (Classe) is a factor variable with 5 leves. We pretend to use the data collected by different weareables in order to train 3 models. These models were trained using 5-fold Cross-Validation and we have tested the trained models with 20% if the data collected as testing sample. The 3 prediction functions used were: Random Forest, a boosted predictor and a linear discriminant analysis. 


## Loading data
First of all, we are going to load the datasets, we have indicated the different possibles outliers (NAs). We have also randomly sorted the data in order to break any possible in-row dependecy.
```{r cache=TRUE}
dat<-read.csv(file="pml-training.csv",sep=",",na.strings = c("NA",""))
datAux<-dat
set.seed(69)
dat<-dat[sample(nrow(dat)),]
validation<-read.csv(file="pml-testing.csv",sep=",",na.strings = c("NA",""))
```

## NA Function with threshold
We have implemented a function wich returns the columns that have a higher ratio of outliers than the threshold indicated.
```{r NAfunction}
NAfunction<-function(df1,threshold){
    res<-c()
    for(i in 1:dim(df1)[2]){
        aux<-sum(is.na(df1[,i]))
        if((aux/dim(df1)[1])>=threshold){
            res<-c(res,i)
        }
    }
   res 
}
```


## Data Exploration
We have broke the dataset in to 2 samples, a training set (80% of the total set) and a test set.
```{r cache=TRUE}
inTrain <- createDataPartition(y=dat$classe, p=0.8, list=FALSE)
set.seed(323)
training<-dat[inTrain,]
testing<-dat[-inTrain,]
nzv<-nearZeroVar(training)
training<-training[,-nzv]
elNA<-NAfunction(training,0.5)
training<-training[,-elNA]
testing<-testing[,-nzv]
testing<-testing[,-elNA]
validation<-validation[,-nzv]
validation<-validation[,-elNA]
skimmed <- skim_to_wide(training)
skimmed[, c(1:5, 9:11, 13, 15:16)]
```
We have eliminated the variables that had near to zero variability as well as the variables that had a ratio of outlier  higher than 50%. We have eliminated these variables in the 3 datasets.

## Training
We are first going to train a Random Forest algorithm with a 5-fold Cross-Validation
```{r RandomForest, cache=TRUE}
fitControl1 <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)

modRF <- train(classe~., method="rf",data=training,trControl = fitControl1,preProcess=c("center","scale"))

```

We are first going to train a boosted predictor algorithm with a 5-fold Cross-Validation
```{r BoostedTrees, cache=TRUE,results="hide"}
fitControlGBM <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)

modGBM <- train(classe~., method="gbm",data=training,trControl = fitControlGBM,preProcess=c("center","scale"))
```

We are first going to train a linear discriminant analysis algorithm with a 5-fold Cross-Validation
```{r LinearDiscriminantAnalysis, cache=TRUE, results="hide", message=FALSE,}
fitControlLDA <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)

modLDA <- train(classe~., method="lda",data=training,trControl = fitControlLDA,preProcess=c("center","scale"))
```

## testing
We are now going to test all the models with the testing part.
```{r TestingChunk}
predRF<-predict(modRF,newdata=testing)
predGBM<-predict(modGBM,newdata=testing)
predLDA<-predict(modLDA,newdata=testing)
confusionMatrix(predRF,testing$classe)$overall
confusionMatrix(predGBM,testing$classe)$overall
confusionMatrix(predLDA,testing$classe)$overall
```
As we can see, the three models seem to perform equally good.

## Validation
We are going to test the three models with the validation data set that was provided
```{r ValidationChunk}
predict(modRF,newdata=validation)
predict(modGBM,newdata=validation)
predict(modLDA,newdata=validation)
```